import org.apache.spark.sql.{SparkSession, SaveMode, DataFrame}
import org.apache.spark.sql.functions._
import java.util.Properties
import scala.util.{Try, Success, Failure}

object OracleToIcebergAdvanced {

def main(args: Array[String]): Unit = {

```
// ì„¤ì • íŒŒë¼ë¯¸í„°
val config = Config(
  oracleUrl = "jdbc:oracle:thin:@//hostname:1521/service_name",
  oracleUser = "your_username",
  oraclePassword = "your_password",
  sourceSchema = "SCHEMA_NAME",
  sourceTable = "TABLE_NAME",
  icebergDatabase = "iceberg_db",
  icebergTable = "migrated_table",
  partitionColumn = Some("created_date"), // íŒŒí‹°ì…˜ ì»¬ëŸ¼ (ì˜µì…˜)
  numPartitions = 8, // ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒí‹°ì…˜ ìˆ˜
  incrementalMode = false, // ì¦ë¶„ ì²˜ë¦¬ ì—¬ë¶€
  incrementalColumn = "updated_at" // ì¦ë¶„ ì²˜ë¦¬ ì»¬ëŸ¼
)

// Spark Session ìƒì„±
val spark = createSparkSession()

try {
  val migrator = new IcebergMigrator(spark, config)
  migrator.migrate()
} catch {
  case e: Exception =>
    println(s"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: ${e.getMessage}")
    e.printStackTrace()
    System.exit(1)
} finally {
  spark.stop()
}
```

}

def createSparkSession(): SparkSession = {
SparkSession.builder()
.appName(â€œOracle to Iceberg V2 Migration - Advancedâ€)
.config(â€œspark.sql.extensionsâ€, â€œorg.apache.iceberg.spark.extensions.IcebergSparkSessionExtensionsâ€)
.config(â€œspark.sql.catalog.spark_catalogâ€, â€œorg.apache.iceberg.spark.SparkCatalogâ€)
.config(â€œspark.sql.catalog.spark_catalog.typeâ€, â€œhiveâ€)
.config(â€œspark.sql.catalog.spark_catalog.uriâ€, â€œthrift://localhost:9083â€)
.config(â€œspark.sql.warehouse.dirâ€, â€œ/user/hive/warehouseâ€)
// Iceberg ìµœì í™” ì„¤ì •
.config(â€œspark.sql.iceberg.handle-timestamp-without-timezoneâ€, â€œtrueâ€)
.config(â€œspark.sql.sources.partitionOverwriteModeâ€, â€œdynamicâ€)
// ì„±ëŠ¥ ìµœì í™”
.config(â€œspark.sql.adaptive.enabledâ€, â€œtrueâ€)
.config(â€œspark.sql.adaptive.coalescePartitions.enabledâ€, â€œtrueâ€)
.enableHiveSupport()
.getOrCreate()
}
}

case class Config(
oracleUrl: String,
oracleUser: String,
oraclePassword: String,
sourceSchema: String,
sourceTable: String,
icebergDatabase: String,
icebergTable: String,
partitionColumn: Option[String],
numPartitions: Int,
incrementalMode: Boolean,
incrementalColumn: String
)

class IcebergMigrator(spark: SparkSession, config: Config) {

import spark.implicits._

private val sourceFullTable = sâ€${config.sourceSchema}.${config.sourceTable}â€
private val icebergFullTable = sâ€${config.icebergDatabase}.${config.icebergTable}â€

def migrate(): Unit = {
println(sâ€=== Oracle to Iceberg V2 ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘ ===â€)
println(sâ€ì†ŒìŠ¤: $sourceFullTableâ€)
println(sâ€íƒ€ê²Ÿ: $icebergFullTableâ€)

```
// 1. Oracleì—ì„œ ë°ì´í„° ì½ê¸°
val sourceDF = readFromOracle()

// 2. ë°ì´í„° ê²€ì¦
validateData(sourceDF)

// 3. Iceberg ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
createIcebergDatabase()

// 4. Iceberg í…Œì´ë¸”ë¡œ ë°ì´í„° ì“°ê¸°
if (config.incrementalMode) {
  writeIncrementalToIceberg(sourceDF)
} else {
  writeFullToIceberg(sourceDF)
}

// 5. ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦
verifyMigration(sourceDF)

println(s"=== ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ ===")
```

}

private def readFromOracle(): DataFrame = {
println(sâ€\nğŸ“¥ Oracle í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì½ê¸°â€¦â€)

```
val connectionProperties = new Properties()
connectionProperties.put("user", config.oracleUser)
connectionProperties.put("password", config.oraclePassword)
connectionProperties.put("driver", "oracle.jdbc.driver.OracleDriver")
connectionProperties.put("fetchsize", "10000")
connectionProperties.put("sessionInitStatement", 
  "BEGIN execute immediate 'alter session set NLS_DATE_FORMAT=\"YYYY-MM-DD HH24:MI:SS\"'; END;")

// ì¦ë¶„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±
val query = if (config.incrementalMode) {
  val lastProcessedValue = getLastProcessedValue()
  lastProcessedValue match {
    case Some(value) =>
      s"(SELECT * FROM $sourceFullTable WHERE ${config.incrementalColumn} > TO_TIMESTAMP('$value', 'YYYY-MM-DD HH24:MI:SS')) as incremental_data"
    case None =>
      s"(SELECT * FROM $sourceFullTable) as full_data"
  }
} else {
  s"(SELECT * FROM $sourceFullTable) as full_data"
}

// ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒí‹°ì…”ë‹ (ìˆ«ì ì»¬ëŸ¼ í•„ìš”)
val df = Try {
  spark.read
    .option("numPartitions", config.numPartitions.toString)
    .jdbc(config.oracleUrl, query, connectionProperties)
} match {
  case Success(df) => df
  case Failure(e) =>
    println(s"âš ï¸  ë³‘ë ¬ ì½ê¸° ì‹¤íŒ¨, ë‹¨ì¼ íŒŒí‹°ì…˜ìœ¼ë¡œ ì¬ì‹œë„...")
    spark.read.jdbc(config.oracleUrl, query, connectionProperties)
}

println(s"âœ“ ì½ì–´ì˜¨ ë ˆì½”ë“œ ìˆ˜: ${df.count()}")
println(s"âœ“ íŒŒí‹°ì…˜ ìˆ˜: ${df.rdd.getNumPartitions}")

df
```

}

private def validateData(df: DataFrame): Unit = {
println(sâ€\nğŸ” ë°ì´í„° ê²€ì¦ ì¤‘â€¦â€)

```
// ìŠ¤í‚¤ë§ˆ ì¶œë ¥
println("ìŠ¤í‚¤ë§ˆ:")
df.printSchema()

// ê¸°ë³¸ í†µê³„
val rowCount = df.count()
println(s"ì´ ë ˆì½”ë“œ ìˆ˜: $rowCount")

if (rowCount == 0) {
  println("âš ï¸  ê²½ê³ : ì½ì–´ì˜¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
}

// NULL ê°’ ì²´í¬ (ì¤‘ìš” ì»¬ëŸ¼)
val nullCounts = df.columns.map { col =>
  col -> df.filter(df(col).isNull).count()
}.filter(_._2 > 0)

if (nullCounts.nonEmpty) {
  println("\nNULL ê°’ì´ ìˆëŠ” ì»¬ëŸ¼:")
  nullCounts.foreach { case (col, count) =>
    println(s"  - $col: $countê°œ")
  }
}

// ìƒ˜í”Œ ë°ì´í„°
println("\nìƒ˜í”Œ ë°ì´í„°:")
df.show(5, truncate = false)
```

}

private def createIcebergDatabase(): Unit = {
println(sâ€\nğŸ—‚ï¸  Iceberg ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±â€¦â€)
spark.sql(sâ€CREATE DATABASE IF NOT EXISTS ${config.icebergDatabase}â€)
spark.sql(sâ€USE ${config.icebergDatabase}â€)
println(sâ€âœ“ ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ: ${config.icebergDatabase}â€)
}

private def writeFullToIceberg(df: DataFrame): Unit = {
println(sâ€\nğŸ’¾ Iceberg V2 í…Œì´ë¸”ë¡œ ì „ì²´ ë°ì´í„° ì“°ê¸°â€¦â€)

```
val writer = df.writeTo(icebergFullTable)
  .using("iceberg")
  .tableProperty("format-version", "2")
  .tableProperty("write.format.default", "parquet")
  .tableProperty("write.parquet.compression-codec", "snappy")
  .tableProperty("write.metadata.compression-codec", "gzip")
  // ìµœì í™” ì„¤ì •
  .tableProperty("write.target-file-size-bytes", "536870912") // 512MB
  .tableProperty("write.parquet.page-size-bytes", "1048576") // 1MB

// íŒŒí‹°ì…˜ ì„¤ì •
config.partitionColumn match {
  case Some(col) =>
    println(s"âœ“ íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì„¤ì •: $col")
    writer.partitionedBy(column(col)).createOrReplace()
  case None =>
    writer.createOrReplace()
}

println(s"âœ“ Iceberg V2 í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
```

}

private def writeIncrementalToIceberg(df: DataFrame): Unit = {
println(sâ€\nğŸ’¾ Iceberg í…Œì´ë¸”ë¡œ ì¦ë¶„ ë°ì´í„° ì¶”ê°€â€¦â€)

```
val tableExists = Try(spark.table(icebergFullTable)).isSuccess

if (tableExists) {
  // í…Œì´ë¸”ì´ ì¡´ì¬í•˜ë©´ append
  df.writeTo(icebergFullTable)
    .using("iceberg")
    .append()
  
  println(s"âœ“ ì¦ë¶„ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")
  
  // ë§ˆì§€ë§‰ ì²˜ë¦¬ ê°’ ì—…ë°ì´íŠ¸
  updateLastProcessedValue(df)
} else {
  // í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ì „ì²´ ì“°ê¸°
  println("âš ï¸  í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì „ì²´ ì“°ê¸° ëª¨ë“œë¡œ ì „í™˜")
  writeFullToIceberg(df)
}
```

}

private def getLastProcessedValue(): Option[String] = {
// ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë³„ë„ì˜ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”ì´ë‚˜ íŒŒì¼ì—ì„œ ì½ì–´ì˜¤ê¸°
// ì—¬ê¸°ì„œëŠ” Iceberg í…Œì´ë¸”ì˜ ìµœëŒ€ê°’ ì‚¬ìš©
Try {
spark.table(icebergFullTable)
.selectExpr(sâ€MAX(${config.incrementalColumn}) as max_valueâ€)
.first()
.getAs[String]("max_value")
}.toOption.flatten
}

private def updateLastProcessedValue(df: DataFrame): Unit = {
val maxValue = df.selectExpr(sâ€MAX(${config.incrementalColumn}) as max_valueâ€)
.first()
.getAs[String]("max_value")

```
println(s"âœ“ ë§ˆì§€ë§‰ ì²˜ë¦¬ ê°’: $maxValue")
// ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë©”íƒ€ë°ì´í„° í…Œì´ë¸”ì— ì €ì¥
```

}

private def verifyMigration(sourceDF: DataFrame): Unit = {
println(sâ€\nâœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì¤‘â€¦â€)

```
val icebergDF = spark.table(icebergFullTable)
val sourceCount = sourceDF.count()
val targetCount = icebergDF.count()

println(s"ì†ŒìŠ¤ ë ˆì½”ë“œ ìˆ˜: $sourceCount")
println(s"íƒ€ê²Ÿ ë ˆì½”ë“œ ìˆ˜: $targetCount")

if (config.incrementalMode) {
  println(s"âœ“ ì¦ë¶„ ì²˜ë¦¬ ëª¨ë“œ - íƒ€ê²Ÿ ë ˆì½”ë“œê°€ ë” ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
} else if (sourceCount == targetCount) {
  println(s"âœ“ ë ˆì½”ë“œ ìˆ˜ ì¼ì¹˜")
} else {
  println(s"âš ï¸  ê²½ê³ : ë ˆì½”ë“œ ìˆ˜ ë¶ˆì¼ì¹˜!")
}

// í…Œì´ë¸” ì†ì„± í™•ì¸
println("\nIceberg í…Œì´ë¸” ì†ì„±:")
spark.sql(s"SHOW TBLPROPERTIES $icebergFullTable")
  .filter("key like '%format%' or key like '%compression%'")
  .show(false)

// ìŠ¤ëƒ…ìƒ· ì •ë³´
println("\nIceberg ìŠ¤ëƒ…ìƒ· ì •ë³´:")
spark.sql(s"SELECT * FROM ${config.icebergDatabase}.${config.icebergTable}.snapshots")
  .show(false)
```

}
}
